[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "World Happiness Report (WHR) Analysis",
    "section": "",
    "text": "Happiness is a key measure of societal well-being, influenced by both economic (e.g., GDP per capita) and non-economic factors (e.g., social support, freedom, and governance). However, understanding how these factors interact remains a challenge.\nThis project aims to analyze the strongest predictors of happiness using data from the World Happiness Report and develop an interactive visualization tool to explore these relationships. By making happiness data more accessible and insightful, this study supports evidence-based decision-making for policymakers and researchers seeking to enhance well-being worldwide."
  },
  {
    "objectID": "proposal.html#exploratory-data-analysis-eda",
    "href": "proposal.html#exploratory-data-analysis-eda",
    "title": "World Happiness Report (WHR) Analysis",
    "section": "4.1 Exploratory Data Analysis (EDA)",
    "text": "4.1 Exploratory Data Analysis (EDA)\nEDA is the first step in understanding the structure and patterns within the World Happiness Report dataset. The primary objectives of EDA in this project include:\nDescriptive Statistics: Summarizing key variables such as Happiness Score, GDP per Capita, Social Support, and Life Expectancy to understand their distribution and central tendencies\n-   **Missing Values**\n\n    -   3 missing values in **GDP per capita, Social Support, Healthy Life Expectancy, Freedom to Make Life Choices, Generosity, Perceptions of Corruption, and Dystopia + Residual**.\n\n    -   No missing values in **Country Name and Ladder Score**\n\n-   **Summary Statistics**\n\n    -   **Ladder Score (Happiness Index)** ranges from **1.72** (least happy) to **7.74** (most happy), with an average of **5.53**.\n\n    -   **Log GDP per capita** varies from **0.00** to **2.14**, implying diverse economies.\n\n    -   **Social Support** has a strong influence, ranging from **0.00** to **1.61**.\n\n    -   **Freedom to Make Life Choices** ranges from **0.00** to **0.86**, showing variation in personal freedoms.\n\n    -   **Generosity & Perceptions of Corruption** have small values, indicating low levels of trust in many countries.\n\nUnivariate & Bivariate Visualizations:\n\nHistograms & Density Plots – Distribution of Happiness Scores\n\n\n\nknitr::include_graphics('plots/density-plots1.png')\n\n\n\n\n\n\n\n\n\nknitr::include_graphics('plots/density-plots2.png')\n\n\n\n\n\n\n\n\nKey Insights from Distributions: Ladder Score (Happiness Index):\n\nPeaks around 5-6, meaning most countries fall within this range. Left-skewed, indicating some countries have very low happiness scores. GDP per Capita & Social Support:\nFollows a normal distribution, with most values around 1.0 - 1.5 for GDP. Social support has a strong right tail, meaning some countries have very high support. Freedom to Make Life Choices & Generosity:\nFreedom has a broad distribution but peaks around 0.6 - 0.7. Generosity is skewed left, meaning most values are quite low. Perceptions of Corruption:\nHighly skewed left, showing many countries perceive corruption as a major issue.\nBoxplots – Comparison of happiness scores across regions\n\n\nhappiness_data &lt;- read_excel(file_path) %&gt;% clean_names()  # Clean column names\n\n# Generate a Boxplot for Happiness Scores across Years\nggplot(happiness_data, aes(x = as.factor(year), y = life_ladder)) + \n  geom_boxplot(fill = \"skyblue\", color = \"black\") + \n  labs(title = \"Happiness Score Distribution by Year\",\n       x = \"Year\",\n       y = \"Happiness Score (Life Ladder)\") + \n  theme_minimal()\n\n\n\n\n\n\n\n# Generate a Boxplot for GDP per Capita across Years\nggplot(happiness_data, aes(x = as.factor(year), y = log_gdp_per_capita)) + \n  geom_boxplot(fill = \"lightgreen\", color = \"black\") + \n  labs(title = \"GDP per Capita Distribution by Year\",\n       x = \"Year\",\n       y = \"Log GDP per Capita\") + \n  theme_minimal()\n\n\n\n\n\n\n\n# Generate a Boxplot for Social Support across Years\nggplot(happiness_data, aes(x = as.factor(year), y = social_support)) + \n  geom_boxplot(fill = \"lightcoral\", color = \"black\") + \n  labs(title = \"Social Support Distribution by Year\",\n       x = \"Year\",\n       y = \"Social Support\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nCorrelation Analysis\nTo identify which factors have the strongest relationship with happiness.\no   *Measure the Strength of Relationships* – Identify which factors (GDP per capita, social support, freedom, generosity, corruption perception, etc.) have the strongest positive or negative correlation with happiness.\n\no   *Detect Key Influencers* – Determine whether economic factors (e.g., GDP per capita) or social indicators (e.g., social support, freedom) have a greater impact on happiness.\n\nknitr::include_graphics('plots/correlation-heatmap.png')\n\n\n\n\n\n\n\n\nCorrelation Insights:\nHappiness (Ladder Score) has the strongest correlations with:\n\nSocial Support (0.81) → Countries with stronger support systems are generally happier. GDP per Capita (0.77) → Wealthier countries tend to be happier. Healthy Life Expectancy (0.76) → Better health leads to higher happiness. Weak or Negative Correlations:\nGenerosity (0.13) and Perceptions of Corruption (0.45) have low influence on happiness. Dystopia + Residual (0.53) captures unexplained variance, slightly correlated.\nGeospatial Analysis:\n\nMapping happiness scores across countries to highlight global patterns\no   *Identify Global Happiness Trends* – Highlight regions with consistently high or low happiness levels, revealing geographical clusters of well-being.\n\no   *Compare Regional Differences* – Examine how happiness varies across continents and correlate findings with economic, social, and governance factors."
  },
  {
    "objectID": "proposal.html#confirmatory-data-analysis-cda",
    "href": "proposal.html#confirmatory-data-analysis-cda",
    "title": "World Happiness Report (WHR) Analysis",
    "section": "4.2 Confirmatory Data Analysis (CDA)",
    "text": "4.2 Confirmatory Data Analysis (CDA)\nRegression Analysis: Objective: Identify the strongest predictors of happiness by modeling the relationship between the dependent variable (Happiness Score/Life Ladder) and independent variables (e.g., GDP per capita, social support, freedom, corruption perception, etc.).\nMethod: Use Multiple Linear Regression (MLR) to quantify the impact of each factor on happiness scores. Perform Stepwise Regression to refine the model and select only the most significant predictors. Check for multicollinearity using Variance Inflation Factor (VIF) to ensure predictor variables are independent.\nOutcome: A predictive model that explains which factors contribute the most to national happiness.\nFeature Importance Analysis: Objective: Understand which variables influence happiness the most when applying non-linear models.\nMethod:\nTrain Random Forest Regression and compute feature importance scores. Use SHAP (SHapley Additive exPlanations) values to interpret how each feature contributes to happiness. Compare machine learning models (Random Forest, Gradient Boosting, etc.) to check if non-linear relationships exist.\nOutcome: A ranked list of happiness predictors, identifying both strong contributors and lesser influential factors.\nClustering Analysis: Objective: Group countries based on similarities in happiness determinants to uncover hidden patterns.\nMethod:\nUse K-Means Clustering to categorize countries into groups based on economic, social, and governance factors. Apply Hierarchical Clustering to visualize the relationships between similar countries. Perform PCA (Principal Component Analysis) to reduce dimensionality and highlight key differences.\nOutcome: Clusters of countries with similar happiness profiles, allowing for comparative regional analysis.\nLongitudinal Analysis: Objective: Examine how happiness scores evolve over time and assess the impact of global events (e.g., economic crises, political shifts, COVID-19 pandemic).\nMethod:\nUse Trend Analysis with LOESS Smoothing to visualize long-term patterns. Apply ARIMA (AutoRegressive Integrated Moving Average) or Exponential Smoothing to forecast future happiness trends. Conduct Interrupted Time Series Analysis (ITSA) to measure the impact of specific events on happiness scores.\nOutcome: Insights into how happiness levels have changed across different periods and predictions for future trends."
  },
  {
    "objectID": "Minutes of Meeting/minutes_of_meeting.html",
    "href": "Minutes of Meeting/minutes_of_meeting.html",
    "title": "ISSS608 Group 14 Project Meeting Minutes Records",
    "section": "",
    "text": "Project Meeting 1: Project Proposal, Project Methodology, Project Timeline\nDate: 05/03/2025\nTime: 8.00pm – 10.00pm\nIn Attendance: Andrea YEO Si Han, Dhreeti SHAH, OU Yi Ming\nClick here to view meeting minutes - 1.\nProject Meeting 2: Data Cleaning, Dashboard Design, Individual Task Assignment, Prototyping & Timeline Planning\nDate: 20/03/2025\nTime: 6.30pm – 8.30pm\nIn Attendance: Andrea YEO Si Han, Dhreeti SHAH, OU Yi Ming\nClick here to view meeting minutes - 2.\nProject Meeting 3: Refining of Analysis Techniques, Finalizing of Dashboard Design, Updating of Individual Progress, Resolving of any bottlenecks, Setting of Final Deadlines\nDate: 29/03/2025\nTime: 4.00pm – 6.00pm\nIn Attendance: Andrea YEO Si Han, Dhreeti SHAH, OU Yi Ming\nClick here to view meeting minutes - 3."
  },
  {
    "objectID": "Minutes of Meeting/meeting2.html",
    "href": "Minutes of Meeting/meeting2.html",
    "title": "ISSS608 Group 14 Meeting Minutes 2",
    "section": "",
    "text": "Project Meeting 2: Data cleaning, Dashboard Design, Individual tasks\nDate: 20/03/2025\nTime: 6.30pm – 8.30pm\nIn Attendance: Andrea YEO Si Han, Dhreeti SHAH, OU Yi Ming\nMinutes Taker: Dhreeti SHAH"
  },
  {
    "objectID": "Minutes of Meeting/meeting2.html#assigning-responsiblites-and-justification",
    "href": "Minutes of Meeting/meeting2.html#assigning-responsiblites-and-justification",
    "title": "ISSS608 Group 14 Meeting Minutes 2",
    "section": "3.1 Assigning Responsiblites and Justification",
    "text": "3.1 Assigning Responsiblites and Justification\nYi Ming (Data Prep & EDA): Volunteered for data preparation and exploratory data analysis, citing a strong background in data wrangling and visualization. Felt comfortable working with raw datasets and uncovering insights through distributions and correlations.\nDhreeti Shah (CDA & Time Series): Chose confirmatory data analysis and time series due to prior experience with statistical modeling. Preferred working with hypothesis testing and tracking trends over time to assess patterns in happiness scores.Also was comfortable with what was taught in class and was able to grasps the concepts.\nAndrea YEO Si Han (Geospatial): Opted for geospatial analysis and clustering because of familiarity with spatial mapping tools. Had previous experience in working with geographic datasets and felt confident in segmenting countries into meaningful groups."
  },
  {
    "objectID": "Minutes of Meeting/meeting2.html#dashboard-design-and-components",
    "href": "Minutes of Meeting/meeting2.html#dashboard-design-and-components",
    "title": "ISSS608 Group 14 Meeting Minutes 2",
    "section": "4.1 Dashboard design and components",
    "text": "4.1 Dashboard design and components\nYi Ming Suggested an interactive EDA dashboard to display happiness score distributions and explore relationships between key factors such as GDP, social support, and freedom. The goal is to provide an intuitive way to observe how different variables interact, using visualizations like histograms, scatterplots, and correlation heatmaps. This step is critical because it lays the foundation for deeper analyses by identifying patterns, potential outliers, and trends before applying advanced models.\nDhreeti Shah suggested a confirmatory data analysis and time series dashboard to validate trends and test statistical significance. This dashboard will allow users to compare happiness score changes over the years and determine which factors have the strongest influence. The reasoning behind this is to ensure that observed patterns are not coincidental but statistically meaningful. Line charts and regression models will be included to visualize long-term trends and relationships. This is crucial for understanding how global happiness has evolved and what factors consistently impact it.\nAndrea YEO Si Han Proposed an interactive geospatial dashboard featuring a world map where users can visualize happiness scores across different regions. Additionally, the clustering dashboard will group similar countries based on happiness-related factors. The rationale behind this is that happiness is highly dependent on geographic and cultural factors, making spatial analysis essential. Mapping allows for an easy, intuitive comparison between countries, while clustering reveals hidden groupings that may not be obvious through numerical analysis alone. Together, these visualizations help in drawing regional insights and making data-driven policy recommendations."
  },
  {
    "objectID": "Minutes of Meeting/meeting2.html#data-cleaning-and-merging-strategy",
    "href": "Minutes of Meeting/meeting2.html#data-cleaning-and-merging-strategy",
    "title": "ISSS608 Group 14 Meeting Minutes 2",
    "section": "5.1 Data Cleaning and Merging Strategy",
    "text": "5.1 Data Cleaning and Merging Strategy\nYi Ming identified missing values, duplicate records, and inconsistent column names, proposing standardization and imputation methods.\nDhreeti Shah recommended merging yearly datasets into a single structured dataset to facilitate time series analysis while ensuring consistency.\nAndrea YEO Si Han suggested normalizing and scaling data for clustering and verifying geographic coordinates for accurate geospatial representation."
  },
  {
    "objectID": "Minutes of Meeting/meeting2.html#reviewing-and-next-steps",
    "href": "Minutes of Meeting/meeting2.html#reviewing-and-next-steps",
    "title": "ISSS608 Group 14 Meeting Minutes 2",
    "section": "6.1 Reviewing and next steps",
    "text": "6.1 Reviewing and next steps\nSetting the Deadline:\nWe collectively decided on March 28th as the deadline for completing individual components. This ensures we have enough time before the final submission deadline on April 2nd to integrate our different analyses into a cohesive Shiny app, conduct testing, and make final adjustments. The reasoning behind this was:\nAvoiding last-minute debugging issues.\nEnsuring time for reviewing each component and resolving inconsistencies between dashboards.\nAllowing buffer time for unexpected issues, particularly in data merging and UI integration.\nDiscussion on Prototyping Tools: To efficiently prototype and visualize our dashboards before full implementation, we discussed several options, including:\nFigma: Considered for UI/UX planning but was ruled out since our focus was more on functionality rather than design-heavy components.\nShiny Sketchboard: A simple sketching tool for planning app layout, which was useful for rough component placements.\nR Markdown & Quarto: Chosen for early-stage visualization testing because it allowed us to quickly generate and refine static graphs before incorporating them into Shiny.\nGoogle Colab & Jupyter Notebooks: Used for preliminary analysis but not for final visualization since we needed an interactive dashboard.\nFinal Decision:\nWe agreed to use Quarto for initial visualization prototypes since it aligns with R’s ecosystem and is easy to translate into Shiny components.\nThe Shiny framework will be used for the final app development, ensuring smooth interactive capabilities.\nGitHub will be used for version control, allowing seamless integration of each person’s work while keeping track of changes.\nNext Steps:\nMarch 10-20: Each person finalizes their individual analysis and data transformations.\nMarch 21-28: Dashboards are developed separately but using a consistent design structure.\nMarch 28-30: Team integrates dashboards, ensuring compatibility and a smooth user experience.\nMarch 30-April 2: Final testing, bug fixes, and refinements before submission.\nMeeting Adjourned."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "World Happiness Visualizations",
    "section": "",
    "text": "“There are so many beautiful things to be happy about.”\n\n\nA gentle reminder as we explore what happiness looks like around the world 🌍"
  },
  {
    "objectID": "acknowledgement.html",
    "href": "acknowledgement.html",
    "title": "Acknowledgment",
    "section": "",
    "text": "About this project\nWe would like to express our deepest gratitude to Prof. Kam for his invaluable teachings and insightful advice throughout this project. His guidance has greatly enriched our understanding and helped refined our approach, and his support has been instrumental in shaping the outcomes of this work. Huge thank for his encouragement and dedication to fostering learning and growth."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataprep.html",
    "href": "dataprep.html",
    "title": "World Happiness Report (WHR) Data Preparation",
    "section": "",
    "text": "pacman::p_load(tidyverse, janitor, slider)\n\npackage 'bit' successfully unpacked and MD5 sums checked\npackage 'ps' successfully unpacked and MD5 sums checked\npackage 'rematch' successfully unpacked and MD5 sums checked\npackage 'bit64' successfully unpacked and MD5 sums checked\npackage 'prettyunits' successfully unpacked and MD5 sums checked\npackage 'processx' successfully unpacked and MD5 sums checked\npackage 'blob' successfully unpacked and MD5 sums checked\npackage 'data.table' successfully unpacked and MD5 sums checked\npackage 'gargle' successfully unpacked and MD5 sums checked\npackage 'uuid' successfully unpacked and MD5 sums checked\npackage 'cellranger' successfully unpacked and MD5 sums checked\npackage 'ids' successfully unpacked and MD5 sums checked\npackage 'rematch2' successfully unpacked and MD5 sums checked\npackage 'timechange' successfully unpacked and MD5 sums checked\npackage 'systemfonts' successfully unpacked and MD5 sums checked\npackage 'textshaping' successfully unpacked and MD5 sums checked\npackage 'clipr' successfully unpacked and MD5 sums checked\npackage 'vroom' successfully unpacked and MD5 sums checked\npackage 'tzdb' successfully unpacked and MD5 sums checked\npackage 'progress' successfully unpacked and MD5 sums checked\npackage 'callr' successfully unpacked and MD5 sums checked\npackage 'selectr' successfully unpacked and MD5 sums checked\npackage 'conflicted' successfully unpacked and MD5 sums checked\npackage 'dbplyr' successfully unpacked and MD5 sums checked\npackage 'dtplyr' successfully unpacked and MD5 sums checked\npackage 'forcats' successfully unpacked and MD5 sums checked\npackage 'googledrive' successfully unpacked and MD5 sums checked\npackage 'googlesheets4' successfully unpacked and MD5 sums checked\npackage 'haven' successfully unpacked and MD5 sums checked\npackage 'hms' successfully unpacked and MD5 sums checked\npackage 'lubridate' successfully unpacked and MD5 sums checked\npackage 'modelr' successfully unpacked and MD5 sums checked\npackage 'ragg' successfully unpacked and MD5 sums checked\npackage 'readr' successfully unpacked and MD5 sums checked\npackage 'readxl' successfully unpacked and MD5 sums checked\npackage 'reprex' successfully unpacked and MD5 sums checked\npackage 'rstudioapi' successfully unpacked and MD5 sums checked\npackage 'rvest' successfully unpacked and MD5 sums checked\npackage 'xml2' successfully unpacked and MD5 sums checked\npackage 'tidyverse' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\andre_ic0s4j6\\AppData\\Local\\Temp\\RtmpI73nf6\\downloaded_packages\n\n\npackage 'snakecase' successfully unpacked and MD5 sums checked\npackage 'janitor' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\andre_ic0s4j6\\AppData\\Local\\Temp\\RtmpI73nf6\\downloaded_packages\n\n\npackage 'warp' successfully unpacked and MD5 sums checked\npackage 'slider' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\andre_ic0s4j6\\AppData\\Local\\Temp\\RtmpI73nf6\\downloaded_packages"
  },
  {
    "objectID": "dataprep.html#loading-of-required-packages",
    "href": "dataprep.html#loading-of-required-packages",
    "title": "World Happiness Report (WHR) Data Preparation",
    "section": "",
    "text": "pacman::p_load(tidyverse, janitor, slider)\n\npackage 'bit' successfully unpacked and MD5 sums checked\npackage 'ps' successfully unpacked and MD5 sums checked\npackage 'rematch' successfully unpacked and MD5 sums checked\npackage 'bit64' successfully unpacked and MD5 sums checked\npackage 'prettyunits' successfully unpacked and MD5 sums checked\npackage 'processx' successfully unpacked and MD5 sums checked\npackage 'blob' successfully unpacked and MD5 sums checked\npackage 'data.table' successfully unpacked and MD5 sums checked\npackage 'gargle' successfully unpacked and MD5 sums checked\npackage 'uuid' successfully unpacked and MD5 sums checked\npackage 'cellranger' successfully unpacked and MD5 sums checked\npackage 'ids' successfully unpacked and MD5 sums checked\npackage 'rematch2' successfully unpacked and MD5 sums checked\npackage 'timechange' successfully unpacked and MD5 sums checked\npackage 'systemfonts' successfully unpacked and MD5 sums checked\npackage 'textshaping' successfully unpacked and MD5 sums checked\npackage 'clipr' successfully unpacked and MD5 sums checked\npackage 'vroom' successfully unpacked and MD5 sums checked\npackage 'tzdb' successfully unpacked and MD5 sums checked\npackage 'progress' successfully unpacked and MD5 sums checked\npackage 'callr' successfully unpacked and MD5 sums checked\npackage 'selectr' successfully unpacked and MD5 sums checked\npackage 'conflicted' successfully unpacked and MD5 sums checked\npackage 'dbplyr' successfully unpacked and MD5 sums checked\npackage 'dtplyr' successfully unpacked and MD5 sums checked\npackage 'forcats' successfully unpacked and MD5 sums checked\npackage 'googledrive' successfully unpacked and MD5 sums checked\npackage 'googlesheets4' successfully unpacked and MD5 sums checked\npackage 'haven' successfully unpacked and MD5 sums checked\npackage 'hms' successfully unpacked and MD5 sums checked\npackage 'lubridate' successfully unpacked and MD5 sums checked\npackage 'modelr' successfully unpacked and MD5 sums checked\npackage 'ragg' successfully unpacked and MD5 sums checked\npackage 'readr' successfully unpacked and MD5 sums checked\npackage 'readxl' successfully unpacked and MD5 sums checked\npackage 'reprex' successfully unpacked and MD5 sums checked\npackage 'rstudioapi' successfully unpacked and MD5 sums checked\npackage 'rvest' successfully unpacked and MD5 sums checked\npackage 'xml2' successfully unpacked and MD5 sums checked\npackage 'tidyverse' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\andre_ic0s4j6\\AppData\\Local\\Temp\\RtmpI73nf6\\downloaded_packages\n\n\npackage 'snakecase' successfully unpacked and MD5 sums checked\npackage 'janitor' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\andre_ic0s4j6\\AppData\\Local\\Temp\\RtmpI73nf6\\downloaded_packages\n\n\npackage 'warp' successfully unpacked and MD5 sums checked\npackage 'slider' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\andre_ic0s4j6\\AppData\\Local\\Temp\\RtmpI73nf6\\downloaded_packages"
  },
  {
    "objectID": "dataprep.html#importing-of-data",
    "href": "dataprep.html#importing-of-data",
    "title": "World Happiness Report (WHR) Data Preparation",
    "section": "Importing of data",
    "text": "Importing of data\nTo combine all the different years of data in their respective csv files, there is a need to do a column mapping due to the different naming conventions used in the different years. The year was also populated using the file name which contains the year the data was obtained.\nThe following function was created to obtain the different naming convention of column headers across all the years.\n\nfolder &lt;- \"data_files/yearly_data/\"\n\nget_column_headers_by_file &lt;- function(folder_path) {\n  file_list &lt;- list.files(path = folder_path, pattern = \"*.csv\", full.names = TRUE)\n  \n  header_list &lt;- map_dfr(file_list, function(file) {\n    col_names &lt;- names(read_csv(file, n_max = 0))\n    cleaned_names &lt;- janitor::make_clean_names(col_names)\n    tibble(\n      file = basename(file),\n      column = cleaned_names\n    )\n  })\n  \n  return(header_list)\n}\n\nheaders_df &lt;- get_column_headers_by_file(folder)\n\nThe following code chunk was used to standardize the column headers naming convention and populate the year the data was collected.\n\n# Column mapping\ncolumn_mapping &lt;- list(\n  country = c(\"country_name\", \"country\", \"country_or_region\"),\n  ladder_score = c(\"ladder_score\", \"happiness_score\", \"score\"),\n  economy_score = c(\"explained_by_log_gdp_per_capita\", \"explained_by_gdp_per_capita\", \"gdp_per_capita\", \"economy_gdp_per_capita\"),\n  social_score = c(\"explained_by_social_support\", \"social_support\", \"family\"),\n  lifeexpectancy_score = c(\"explained_by_healthy_life_expectancy\", \"healthy_life_expectancy\", \"health_life_expectancy\"),\n  freedom_score = c(\"explained_by_freedom_to_make_life_choices\", \"freedom_to_make_life_choices\", \"freedom\"),\n  generosity_score = c(\"explained_by_generosity\", \"generosity\"),\n  corrperception_score = c(\"explained_by_perceptions_of_corruption\", \"perceptions_of_corruption\", \"trust_government_corruption\"),\n  residual_score = c(\"dystopia_residual\", \"dystopia_1_83_residual\", \"dystopia_residual\")\n)\n\n# Function to clean and standardize columns\nstandardize_columns &lt;- function(df) {\n  df &lt;- df %&gt;% janitor::clean_names()\n\n  for (std_name in names(column_mapping)) {\n    matched_name &lt;- column_mapping[[std_name]]\n    current_names &lt;- names(df)\n    found &lt;- intersect(matched_name %&gt;% tolower(), current_names)\n    if (length(found) &gt; 0) {\n      names(df)[which(names(df) == found[1])] &lt;- std_name\n    }\n  }\n  return(df)\n}\n\n# Function to extract year from filename\nextract_year &lt;- function(file_name) {\n  year_match &lt;- str_extract(file_name, \"\\\\d{4}\")  # Extracts 4-digit year\n  return(as.integer(year_match))\n}\n\n# Function to read and process all CSV files\ncombine_csv_files &lt;- function(folder_path) {\n  file_list &lt;- list.files(path = folder_path, pattern = \"*.csv\", full.names = TRUE)\n  combined_data &lt;- map(file_list, function(file) {\n    df &lt;- read_csv(file, col_types = cols(.default = \"c\")) %&gt;%\n      standardize_columns() %&gt;%\n      mutate(\n        source_file = basename(file),\n        year = extract_year(basename(file)) - 1\n      ) %&gt;%\n      select(any_of(c(\"year\", names(column_mapping))))\n    return(df)\n  }) %&gt;%\n    bind_rows()\n  return(combined_data)\n}\n\nfolder_path &lt;- \"data_files/yearly_data/\"\nraw_data &lt;- combine_csv_files(folder_path)\n\n# Preview\nglimpse(raw_data)\n\nRows: 1,657\nColumns: 10\n$ year                 &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2…\n$ country              &lt;chr&gt; \"Switzerland\", \"Iceland\", \"Denmark\", \"Norway\", \"C…\n$ ladder_score         &lt;chr&gt; \"7.587\", \"7.561\", \"7.527\", \"7.522\", \"7.427\", \"7.4…\n$ economy_score        &lt;chr&gt; \"1.39651\", \"1.30232\", \"1.32548\", \"1.459\", \"1.3262…\n$ social_score         &lt;chr&gt; \"1.34951\", \"1.40223\", \"1.36058\", \"1.33095\", \"1.32…\n$ lifeexpectancy_score &lt;chr&gt; \"0.94143\", \"0.94784\", \"0.87464\", \"0.88521\", \"0.90…\n$ freedom_score        &lt;chr&gt; \"0.66557\", \"0.62877\", \"0.64938\", \"0.66973\", \"0.63…\n$ generosity_score     &lt;chr&gt; \"0.29678\", \"0.4363\", \"0.34139\", \"0.34699\", \"0.458…\n$ corrperception_score &lt;chr&gt; \"0.41978\", \"0.14145\", \"0.48357\", \"0.36503\", \"0.32…\n$ residual_score       &lt;chr&gt; \"2.51738\", \"2.70201\", \"2.49204\", \"2.46531\", \"2.45…"
  },
  {
    "objectID": "dataprep.html#cleaning-of-data",
    "href": "dataprep.html#cleaning-of-data",
    "title": "World Happiness Report (WHR) Data Preparation",
    "section": "Cleaning of Data",
    "text": "Cleaning of Data\nFirst, the missing values for residual_score will be populated by calculating the residual.\n\ncleaned_data &lt;- raw_data %&gt;%\n  mutate(across(any_of(c(\"ladder_score\", \"economy_score\", \"social_score\", \"lifeexpectancy_score\", \"freedom_score\", \"generosity_score\", \"corrperception_score\", \"residual_score\")), as.numeric)) %&gt;%\n  mutate(residual_score = if_else(\n    is.na(residual_score), \n    ladder_score - economy_score - social_score - \n    lifeexpectancy_score - freedom_score - \n    generosity_score - corrperception_score, \n    residual_score\n  ))\n\nNext, the country’s rank in the respective year will be re-populated based on the ladder_score.\n\ncleaned_data &lt;- cleaned_data %&gt;%\n  group_by(year) %&gt;%\n  arrange(desc(ladder_score)) %&gt;%\n  mutate(rank = row_number()) %&gt;%\n  ungroup()\n\nBefore we map the countries to the regions, the countries name is also observed to have different naming conventions which needs to be fixed\n\n# Removal of special characters\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(country = str_replace_all(country, \"\\\\*\", \"\") %&gt;% str_trim())\n\n# Country name matching\ncountry_name_fixes &lt;- c(\n  \"Czech Republic\" = \"Czechia\",\n  \"Congo (Brazzaville)\" = \"Congo\",\n  \"DR Congo\" = \"Congo, Democratic Republic of the\",\n  \"Congo (Kinshasa)\" = \"Congo, Democratic Republic of the\",\n  \"Congo\" = \"Congo, Democratic Republic of the\",\n  \"State of Palestine\" = \"Palestine, State of\",\n  \"Palestinian Territories\" = \"Palestine, State of\",\n  \"Hong Kong SAR of China\" = \"Hong Kong\",\n  \"Hong Kong S.A.R. of China\" = \"Hong Kong\",\n  \"Hong Kong S.A.R., China\" = \"Hong Kong\",\n  \"South Korea\" = \"Korea, Democratic People's Republic of\",\n  \"Republic of Korea\" = \"Korea, Democratic People's Republic of\",\n  \"Macedonia\" = \"North Macedonia\",\n  \"Russia\" = \"Russian Federation\",\n  \"Eswatini, Kingdom of\" = \"Eswatini\",\n  \"Netherlands\" = \"Netherlands, Kingdom of the\",\n  \"United Kingdom\" = \"United Kingdom of Great Britain and Northern Ireland\",\n  \"United States\" = \"United States of America\",\n  \"Venezuela\" = \"Venezuela, Bolivarian Republic of\",\n  \"Taiwan\" = \"Taiwan, Province of China\",\n  \"Taiwan Province of China\" = \"Taiwan, Province of China\",\n  \"Trinidad & Tobago\" = \"Trinidad and Tobago\",\n  \"Vietnam\" = \"Viet Nam\",\n  \"Moldova\" = \"Moldova, Republic of\",\n  \"Republic of Moldova\" = \"Moldova, Republic of\",\n  \"Bolivia\" = \"Bolivia, Plurinational State of\",\n  \"Northern Cyprus\" = \"Cyprus\",\n  \"North Cyprus\" = \"Cyprus\",\n  \"Turkey\" = \"Türkiye\",\n  \"Turkiye\" = \"Türkiye\",\n  \"Ivory Coast\" = \"Côte d'Ivoire\",\n  \"Côte d’Ivoire\" = \"Côte d'Ivoire\",\n  \"Laos\" = \"Lao People's Democratic Republic\",\n  \"Lao PDR\" = \"Lao People's Democratic Republic\",\n  \"Iran\" = \"Iran, Islamic Republic of\",\n  \"Swaziland\" = \"Eswatini\",\n  \"Tanzania\" = \"Tanzania, United Republic of\",\n  \"Syria\" = \"Syrian Arab Republic\",\n  \"Somaliland region\" = \"Somaliland Region\"\n)\n\n# Apply mapping\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(country_recode = recode(country, !!!country_name_fixes))\n\nCountry-region mapping will be also be implemented for future geographical analysis. The mapping will be obtained from a github url.\n\nurl &lt;- \"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\"\n\n# Read the CSV file into a DataFrame\ncountry_region_mapping &lt;- read_csv(url)\n\n# Map the cleaned country names to its region\ncleaned_data &lt;- cleaned_data %&gt;%\n  left_join(country_region_mapping %&gt;% select(name, region), by = c(\"country_recode\" = \"name\")) %&gt;%\n  mutate(region = if_else(is.na(region), \"Unknown\", region))\n\n# Additional country_region_mapping\ncountry_region &lt;- c(\n  \"Kosovo\" = \"Europe\",\n  \"Somaliland Region\" = \"Africa\",\n  \"Taiwan, Province of China\" = \"Asia\"\n)\n\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(\n    region = if_else(\n      !is.na(country_region[country_recode]),\n      country_region[country_recode],\n      region))\n\nThe country column is then recoded with the country_recoded column with Cyprus mapped back to either Northern Cyrpus and North Cyprus in the years 2014-2021 accordingly with the country being combined to Cyprus in 2022-2023 as per the original dataset.\n\ncyprus_variant &lt;- cleaned_data %&gt;%\n  filter(country %in% c(\"North Cyprus\", \"Northern Cyprus\")) %&gt;%\n  distinct(year, country) %&gt;%\n  mutate(\n    flipped_variant = case_when(\n      country == \"North Cyprus\" ~ \"Northern Cyprus\",\n      country == \"Northern Cyprus\" ~ \"North Cyprus\"\n    )\n  )%&gt;%\n  select (year, flipped_variant)\n\ncleaned_data &lt;- cleaned_data %&gt;%\n  left_join(cyprus_variant, by = \"year\") %&gt;%\n  mutate(\n    country = if_else(country == \"Cyprus\" & !is.na(flipped_variant),\n                      flipped_variant,\n                      country)\n  ) %&gt;%\n  select(-flipped_variant)\n\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(country = if_else(\n    country %in% c(\"Cyprus\", \"Northern Cyprus\", \"North Cyprus\"),\n    country,\n    country_recode\n  )) %&gt;%\n  select(-country_recode)\n\nAfter all the preliminary cleaning of dataset is performed, a summary of the cleaned_data is shown below.\n\nsummary(cleaned_data)\n\n      year        country           ladder_score   economy_score   \n Min.   :2014   Length:1657        Min.   :1.364   Min.   :0.0000  \n 1st Qu.:2016   Class :character   1st Qu.:4.607   1st Qu.:0.7601  \n Median :2019   Mode  :character   Median :5.500   Median :1.0990  \n Mean   :2019                      Mean   :5.461   Mean   :1.0764  \n 3rd Qu.:2022                      3rd Qu.:6.302   3rd Qu.:1.3948  \n Max.   :2024                      Max.   :7.842   Max.   :2.2090  \n                                                   NA's   :3       \n  social_score    lifeexpectancy_score freedom_score    generosity_score\n Min.   :0.0000   Min.   :0.0000       Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.8636   1st Qu.:0.4020       1st Qu.:0.3794   1st Qu.:0.1069  \n Median :1.1140   Median :0.5970       Median :0.4970   Median :0.1700  \n Mean   :1.0785   Mean   :0.5758       Mean   :0.4915   Mean   :0.1847  \n 3rd Qu.:1.3507   3rd Qu.:0.7600       3rd Qu.:0.6111   3rd Qu.:0.2423  \n Max.   :1.8400   Max.   :1.1410       Max.   :1.0180   Max.   :0.8381  \n NA's   :3        NA's   :5            NA's   :4        NA's   :3       \n corrperception_score residual_score        rank           region         \n Min.   :0.00000      Min.   :-0.110   Min.   :  1.00   Length:1657       \n 1st Qu.:0.05898      1st Qu.: 1.553   1st Qu.: 38.00   Class :character  \n Median :0.10100      Median : 1.923   Median : 76.00   Mode  :character  \n Mean   :0.13541      Mean   : 1.920   Mean   : 75.96                     \n 3rd Qu.:0.17237      3rd Qu.: 2.309   3rd Qu.:113.00                     \n Max.   :0.58700      Max.   : 3.838   Max.   :158.00                     \n NA's   :5            NA's   :8                                           \n\n\nSince Oman only has 2 rows of data in the dataset for 2014 and 2024, it will be more intuitional to remove the data.\n\ncleaned_data %&gt;%\n  filter(if_any(everything(), is.na)) %&gt;%\n  group_by(country) %&gt;%\n  summarise(missing_cols = sum(is.na(across(everything()))))\n\n# A tibble: 5 × 2\n  country              missing_cols\n  &lt;chr&gt;                       &lt;int&gt;\n1 Bahrain                         7\n2 Oman                            2\n3 Palestine, State of            11\n4 Tajikistan                      9\n5 United Arab Emirates            2\n\n\n\ncleaned_data &lt;- cleaned_data %&gt;%\n  filter(country != \"Oman\")\n\nFor each country with missing values in a given year, the missing scores are filled using the mean from the past 3 years for the same country and same column while the residual score is populated as the residual of the ladder score after deducting all the explanatory scores.\n\ncolumns_to_impute &lt;- c(\"generosity_score\", \"freedom_score\", \"social_score\", \"economy_score\", \"corrperception_score\", \"lifeexpectancy_score\")\n\ncleaned_data &lt;- cleaned_data %&gt;%\n  arrange(country, year) %&gt;%\n  group_by(country) %&gt;%\n  mutate(across(\n    all_of(columns_to_impute), \n          ~ if_else(is.na(.),\n                    slide_dbl(.x, mean, .before = 3, .complete = TRUE, na.rm = TRUE),\n                    .)\n  )) %&gt;%\n  ungroup() %&gt;%\n  mutate(residual_score = if_else(\n    is.na(residual_score), \n    ladder_score - economy_score - social_score - \n    lifeexpectancy_score - freedom_score - \n    generosity_score - corrperception_score, \n    residual_score\n  ))\n\nThere is a need to check if there are any countries with more than 10 rows of data given that the number of yearly data obtained is 10.\n\ncleaned_data %&gt;%\n  count(country) %&gt;%\n  filter(n &gt; 10)\n\n# A tibble: 129 × 2\n   country         n\n   &lt;chr&gt;       &lt;int&gt;\n 1 Afghanistan    11\n 2 Albania        11\n 3 Algeria        11\n 4 Argentina      11\n 5 Armenia        11\n 6 Australia      11\n 7 Austria        11\n 8 Bahrain        11\n 9 Bangladesh     11\n10 Belgium        11\n# ℹ 119 more rows\n\n\nEach country should only have one row of data per year as confirmed below.\n\ncleaned_data %&gt;%\n  group_by(country, year) %&gt;%\n  filter(n() &gt; 1)\n\n# A tibble: 0 × 12\n# Groups:   country, year [0]\n# ℹ 12 variables: year &lt;dbl&gt;, country &lt;chr&gt;, ladder_score &lt;dbl&gt;,\n#   economy_score &lt;dbl&gt;, social_score &lt;dbl&gt;, lifeexpectancy_score &lt;dbl&gt;,\n#   freedom_score &lt;dbl&gt;, generosity_score &lt;dbl&gt;, corrperception_score &lt;dbl&gt;,\n#   residual_score &lt;dbl&gt;, rank &lt;int&gt;, region &lt;chr&gt;\n\n\nThe column types are also enforced as below.\n\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(\n    year = as.integer(year),\n    rank = as.integer(rank),\n    country = as.character(country),\n    region = as.factor(region)\n    ) %&gt;%\n  mutate(\n    across(c(ladder_score, economy_score, social_score, lifeexpectancy_score, freedom_score, generosity_score, corrperception_score, residual_score),\n                ~ round(., 5)))\n\nThe cleaned dataset is as shown below.\n\nDT::datatable(cleaned_data, \n          options = list(pageLength = 10, scrollX = TRUE), \n          class = 'cell-border stripe hover',\n          rownames = FALSE)\n\n\n\n\n\n\nwrite_csv(cleaned_data, \"data_files/world_happiness.csv\")"
  },
  {
    "objectID": "Minutes of Meeting/meeting1.html",
    "href": "Minutes of Meeting/meeting1.html",
    "title": "ISSS608 Group 14 Meeting Minutes 1",
    "section": "",
    "text": "Project Meeting 1: Project Proposal, Project Methodology, Project Timeline\nDate: 05/03/2025\nTime: 8.00pm – 10.00pm\nIn Attendance: Andrea YEO Si Han, Dhreeti SHAH, OU Yi Ming\nMinutes Taker: Andrea YEO Si Han"
  },
  {
    "objectID": "Minutes of Meeting/meeting1.html#discussion-on-proposed-topics",
    "href": "Minutes of Meeting/meeting1.html#discussion-on-proposed-topics",
    "title": "ISSS608 Group 14 Meeting Minutes 1",
    "section": "3.1 Discussion on Proposed Topics:",
    "text": "3.1 Discussion on Proposed Topics:\n\n3.1.1 Evaluation of the financial transactions and fraud detection dataset\nDhreeti suggested analyzing financial transactions and fraud detection using a dataset available on Kaggle. She noted that this dataset is well-suited for Exploratory Data Analysis (EDA) and predictive modeling techniques such as classification and anomaly detection. The dataset included various transaction types, timestamps, and fraud indicators, making it a compelling choice for understanding financial fraud patterns.\nGood:\n\nContained diverse transaction types and timestamps.\nSuitable for predictive modeling and anomaly detection.\nCould provide insights into fraud trends and detection patterns.\n\nBad:\n\nData credibility concerns due to the confidential nature of financial transactions.\nLack of transparency in data sourcing and potential synthetic data issues.\nLimited time period (Total: 8 years), reducing the ability to analyze long-term trends.\nNarrow scope for exploratory data analysis beyond anomaly detection. There may or may not be any anomaly in the dataset.\n\n\n\n3.1.2 Evaluation of the World Happiness Report (WHR) dataset\nAndrea introduced the idea of analyzing global happiness trends using data from the World Happiness Report (WHR). The dataset includes economic, social, and governance-related factors that contribute to national well-being. This dataset spans from 2008 to 2014 and is released as an annual report. It provides an opportunity to explore how happiness levels change year over year for different countries and identify patterns over time. By examining key contributing factors such as GDP per capita, social support, life expectancy, freedom, generosity, and perceptions of corruption, this dataset allows for an in-depth analysis of what influences a country’s happiness score. It will be particularly interesting to visualize trends across different regions and observe how certain global or national events impact overall happiness.\nGood:\n\nCovers at least 16 years of data (2008-2024), enabling trend analysis.\nProvides a mix of economic, social, and governance factors, allowing for a multi-dimensional exploration of happiness.\nOpen-source and publicly available, ensuring data credibility.\nAllows for geographic and time-series visualizations.\nPotential for interactive dashboards to explore how different factors influence happiness across countries and over time.\n\nBad:\n\nSome variables may have missing values, requiring data cleaning and imputation.\nLimited number of happiness-related factors (total of 8), which may restrict deeper analysis on additional influences beyond those included.\n\n\n\n3.1.3 Evaluation of the Tourism dataset\nYi Ming suggested analyzing tourism trends using a dataset from the Singapore Department of Statistics (DOS). This dataset contains data on visitor arrivals on a month-to-month basis across countries, hotel occupancy rates, and available room nights. It is suited for seasonal forecasting, trend analysis, and geographic visualization, making it a strong candidate for exploring tourism behavior and economic impact.\nGood:\n\nProvides structured data on international visitor arrivals, hotel occupancy, and tourism trends over time.\nSuitable for seasonal trend analysis and forecasting.\nAllows for geographic and time-series visualizations.\n\nBad:\n\nPrimarily focused on tourism volume without deeper insights into traveler behavior and spending.\nExtensive existing research on tourism spending, making the analysis less original.\n\n\n\n3.1.4 Final Project Selection and Key Areas of Exploration\nAfter considering the different proposed ideas, the team narrowed the options down to two: Tourism Dataset and World Happiness Report (WHR) Dataset. Ultimately, the team unanimously decided on the World Happiness Report (WHR) Analysis due to its novelty and the opportunity to explore and apply the various data visualization techniques learned in class effectively. Using this dataset, the group aims to develop an interactive visual analytics application using Shiny that will allow users to explore happiness trends in a comprehensive and dynamic manner.\n\n\nKey Areas of Exploration:\n\nImpact of Various Factors on Happiness: Analyze how economic, social, and governance-related factors contribute to happiness across different countries and regions.\nTemporal Trends in Happiness: Examine how happiness levels have changed over time from 2008 to 2024 and identify key global events that may have influenced these changes.\nSignificant Predictors of Happiness: Determine which variables have the strongest impact on a country’s happiness score and explore regional differences in their influence."
  },
  {
    "objectID": "Minutes of Meeting/meeting1.html#proposed-methodology---broad-overall-project-approach",
    "href": "Minutes of Meeting/meeting1.html#proposed-methodology---broad-overall-project-approach",
    "title": "ISSS608 Group 14 Meeting Minutes 1",
    "section": "4.1 Proposed Methodology - Broad overall project approach",
    "text": "4.1 Proposed Methodology - Broad overall project approach\nThe team discussed the methodology for conducting the World Happiness Report Analysis, focusing on a structured approach that integrates data preparation, exploratory data analysis, confirmatory analysis, and visualization techniques.\n\n4.1.1 Data preparation\n\nImport and clean the dataset by handling missing values, normalizing data, and ensuring consistency in variable formats.\nRemove redundant or highly correlated variables to avoid overfitting in statistical analysis.\nEncode categorical variables (if needed) for analytical consistency.\nTransform time-series data to facilitate trend analysis.\n\n\n\n4.1.2 Exploratory Data Analysis (EDA) via Data Visualization Methods\nThe goal of EDA is to understand variable distributions, relationships, and trends across different regions and time periods. The team identified key visualization techniques, including:\n\nTime-series analysis:\n\nLine charts & bar plots to show how happiness scores evolve over time per country/region.\nStacked area charts to compare happiness trends across multiple countries.\n\nGeospatial visualization:\n\nChoropleth maps to display global happiness levels by country.\nHeatmaps for regional variations in specific happiness factors (e.g., social support, GDP, governance).\n\nCorrelation and distribution analysis:\n\nBubble plots & scatter plots to analyze relationships between happiness scores and key factors (e.g., GDP per capita, freedom).\nViolin & box plots to compare happiness score distributions across economic classifications.\n\n\n\n\n4.1.3 Confirmatory Data Analysis (CDA) via Statistical Methods**\nTo validate hypotheses and measure the impact of various factors on happiness, statistical methods will be applied:\n\nCorrelation analysis:\n\nPearson/Spearman correlation tests to determine the strength of relationships between happiness and key factors.\nNetwork correlation plots to visualize interdependencies among happiness variables.\n\nRegression modeling:\n\nMultiple Linear Regression (MLR) to quantify how economic, social, and governance factors predict happiness.\nFeature importance analysis using Random Forest or SHAP values to identify the most influential predictors.\n\nComparative analysis:\n\nANOVA (One-Way and Two-Way) tests to assess happiness differences across income groups, regions, or governance levels.\nT-tests & Wilcoxon tests for pairwise country comparisons of happiness scores over time.\n\n\n\n\n4.1.4 Visualization & Dashboard Development\nThe final step involves integrating Shiny to develop an interactive web-based visualization tool that:\n\nAllows users to explore happiness trends by selecting regions, time periods, and key influencing factors.\nProvides real-time filtering and comparison tools for analyzing multiple countries side-by-side.\nIntegrates predictive modeling outputs to showcase which factors most significantly affect happiness scores."
  },
  {
    "objectID": "Minutes of Meeting/meeting3.html",
    "href": "Minutes of Meeting/meeting3.html",
    "title": "ISSS608 Group 14 Meeting Minutes 3",
    "section": "",
    "text": "1. ISSS608 Group 14 Meeting Minutes\nProject Meeting 3: Refining of Analysis Techniques, Finalizing of Dashboard Design, Updating of Individual Progress, Resolving of any bottlenecks, Setting of Final Deadlines\nDate: 29/03/2025\nTime: 4.00pm – 6.00pm\nIn Attendance: Andrea YEO Si Han, Dhreeti SHAH, OU Yi Ming\nMinutes Taker: Ou Yi Ming\n\n\n\n2. Agenda Items\n\nAgenda Item 1: Refining of Analysis Techniques\nAgenda Item 2: Finalizing of Dashboard Design\nAgenda Item 3: Updating of Individual Progress\nAgenda Item 4: Resolving of Bottlenecks\nAgenda Item 5: Setting of Final Deadlines\n\n\n\n\n3. Agenda Item 1: Refining of Analysis Techniques\nThe team discussed and refined the analytical techniques to be applied in the project. The key components to be incorporated into the final analysis include:\n\nTime Series Analysis: To analyze the temporal patterns of happiness scores and influencing factors.\nMultivariate Clustering: To segment countries based on multiple happiness-related variables by years for a better understanding.\nTime Series Forecasting: To predict future happiness trends based on historical patterns.\nTarget Variable Prediction via Input Changes: To allow users to simulate how changes in key variables (e.g., GDP, social support) may affect happiness scores.\nPenalized Regression (e.g., Ridge, LASSO): To handle multicollinearity issues and select relevant predictors for the final modeling.\n\nEach member agreed on the relevance of these methods and committed to incorporating them into their respective analysis components.\n\n\n\n4. Agenda Item 2: Finalizing of Dashboard Design\nThe team finalized the dashboard’s visual structure, focusing on UI design, layout, and color scheme.\n\nThe overall structure will include the following sections:\n\nHome Page\nTime Series Forecasting & Predictive Modeling\nClustering (Time-series and Multivariate)\nGeospatial and Aspatial Analysis\nAbout\n\nThe color scheme will be soft and professional, ensuring visual clarity without distracting users.\nA consistent layout (yellow color scheme) will be used across all dashboards to maintain uniformity.\nThe team agreed to keep the UI user-friendly, with filters, sliders, and dropdowns for interactive exploration.\n\n\n\n\n5. Agenda Item 3: Updating of Individual Progress\nEach member provided an update on their assigned components:\n\nAndrea completed exploratory data analysis (EDA), generated initial geographical plots, and performed Local Indicators of Spatial Association (LISA) analysis.\nDhreeti performed time series forecasting and conducted time series analysis for trend identification.\nYi Ming performed preliminary EDA and explored suitable clustering techniques to apply in the multivariate clustering section.\n\n\n\n\n6. Agenda Item 4: Resolving of Bottlenecks\nA critical issue was identified regarding data inconsistencies due to multiple exports of the cleaned dataset during collaborative work. Specifically, some columns contained unknown or corrupted values.\n\nThe problem was traced to the repeated execution of code chunks that exported the cleaned dataset to CSV.\nThe issue was resolved by:\n\nSetting the export code chunk to eval: false to prevent repeated overwriting.\nCreating a shared cleaned dataset saved in a separate folder as a “read-only” resource for all team members.\n\n\nThis solution ensured consistency across all subsequent analyses.\n\n\n\n7. Agenda Item 5: Setting of Final Deadlines\nThe team reached a consensus on the following timeline for deliverables:\n\n\n\nDeliverable\nDeadline\n\n\n\n\nCombined Shiny App Code\n30/03/2025\n\n\nPoster Completion\n31/03/2025\n\n\nUI Refinement\n01/04/2025\n\n\nUser Guide Completion\n02/04/2025\n\n\n\nThis timeline allows for adequate buffer time before the final project submission on April 5th.\n\nWith all discussion points addressed, the meeting was adjourned at 6:00 PM.\nMeeting Adjourned"
  },
  {
    "objectID": "poster.html",
    "href": "poster.html",
    "title": "ISSS608 Group 14 Poster",
    "section": "",
    "text": "0.1 📥 Download the Poster\n\nClick here to download the poster as PDF.\nClick here to download the poster as PNG."
  }
]